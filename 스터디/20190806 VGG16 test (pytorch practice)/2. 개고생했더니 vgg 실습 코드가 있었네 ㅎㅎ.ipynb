{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"개고생했더니 vgg 실습 코드가 있었네 ㅎㅎ.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"deicoBzX4Bv9","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torch.optim as optim\n","from torch.utils import data\n","from torchvision import datasets, transforms, models\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import copy\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0kZJ83HV374T","colab_type":"text"},"source":["# 이미지 데이터 불러오기"]},{"cell_type":"code","metadata":{"id":"zD-iRixi4cpa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"db950ce4-6694-48a7-c9e5-83c187e939c8","executionInfo":{"status":"ok","timestamp":1565120721339,"user_tz":-540,"elapsed":615,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["# 구글 드라이브를 마운트 한다.\n","\n","from google import colab\n","colab.drive.mount(\"/drive/\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /drive/; to attempt to forcibly remount, call drive.mount(\"/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aacClsCHCEgH","colab_type":"code","outputId":"89b8dfab-78c0-4b96-d64e-aaea692a00bd","executionInfo":{"status":"ok","timestamp":1565120726467,"user_tz":-540,"elapsed":5509,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 구글 드라이브에서 데이터가 있는 path를 찾았다.\n","\n","!ls \"/drive/My Drive/빅데이터 청년인재 고려대 과정 1조/K-Data 고려대학교 빅데이터 청년인재 교육과정 1조/프로젝트/줄기\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["test  train\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lHmf8v3Q_wjC","colab_type":"code","colab":{}},"source":["data_dir = \"/drive/My Drive/빅데이터 청년인재 고려대 과정 1조/K-Data 고려대학교 빅데이터 청년인재 교육과정 1조/프로젝트/줄기\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1o8QA6_DCzp","colab_type":"code","outputId":"d7cb88d4-0378-4270-b917-44fdbd2b625b","executionInfo":{"status":"ok","timestamp":1565120726469,"user_tz":-540,"elapsed":5068,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# path 지정함\n","# os.path.join으로 path 합치는 거 해봄.\n","\n","data_path = {x:os.path.join(data_dir, x) for x in [\"train\", \"test\"]}\n","data_path"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'test': '/drive/My Drive/빅데이터 청년인재 고려대 과정 1조/K-Data 고려대학교 빅데이터 청년인재 교육과정 1조/프로젝트/줄기/test',\n"," 'train': '/drive/My Drive/빅데이터 청년인재 고려대 과정 1조/K-Data 고려대학교 빅데이터 청년인재 교육과정 1조/프로젝트/줄기/train'}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"hC-z7aO3VwPd","colab_type":"code","colab":{}},"source":["# Compose는 조립한다는 뜻이다.\n","# transforms의 함수들을 리스트로 만들어서 인자로 전달하면, pipeline같은 역할을 한다.\n","\n","# data augmentation도 여기서 처리할 수 있다.\n","# 하지만 일단 처음 돌리는거이므로 pass.\n","\n","data_transforms = transforms.Compose([\n","    transforms.CenterCrop(2688), # 중앙을 중심을 크롭해준다.\n","                                 # 인자가 int면 정방형, 가로세로 정하려면 시퀀스로.\n","    transforms.Resize(224),\n","    transforms.ToTensor(), # tensor 객체로 바꿔준다. \n","    transforms.Normalize((0.5, 0.5, 0.5), (0.3, 0.3, 0.3)),# 노말라이즈한다.\n","                                                           # 평균과 분산을 넣는다. 채널 수 만큼 넣는다.\n","                                                           # 일단 아무 숫자나 넣어본다.\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHgz04OlZdvN","colab_type":"code","outputId":"f1d7ba33-2eb1-415a-8110-9f4bba0459bb","executionInfo":{"status":"ok","timestamp":1565120726470,"user_tz":-540,"elapsed":4636,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["# ImageFolder 함수는 path에서 이미지를 load해온다. \n","# ToTensor하기 전에는 PILimage로 가져온다. (이름 정확히 기억 안남)\n","\n","stem_datasets = {x : torchvision.datasets.ImageFolder(data_path[x], data_transforms) \\\n","              for x in [\"train\", 'test']}\n","stem_datasets"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'test': Dataset ImageFolder\n","     Number of datapoints: 120\n","     Root location: /drive/My Drive/빅데이터 청년인재 고려대 과정 1조/K-Data 고려대학교 빅데이터 청년인재 교육과정 1조/프로젝트/줄기/test,\n"," 'train': Dataset ImageFolder\n","     Number of datapoints: 480\n","     Root location: /drive/My Drive/빅데이터 청년인재 고려대 과정 1조/K-Data 고려대학교 빅데이터 청년인재 교육과정 1조/프로젝트/줄기/train}"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"8HKP_AISTiL9","colab_type":"code","outputId":"5d7a4a1d-c0c3-435c-869b-621cec4f710e","executionInfo":{"status":"ok","timestamp":1565120726471,"user_tz":-540,"elapsed":4366,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 학습하려면 DataLoader 타입으로 만들어줘야한다.\n","\n","type(stem_datasets['train'])"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torchvision.datasets.folder.ImageFolder"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"PeKvar2OTo8D","colab_type":"code","outputId":"f7862f67-2403-4ccd-cfba-06fed093448e","executionInfo":{"status":"ok","timestamp":1565120726471,"user_tz":-540,"elapsed":4143,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 폴더 명이 class로 자동 매칭된다.\n","\n","class_names = stem_datasets['train'].classes\n","class_names"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['감수_train', '대극_train', '파극천_train']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"VwjByPEa4sg4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c3dbc2bd-2f49-41a8-8f97-70f2a7ea60cb","executionInfo":{"status":"ok","timestamp":1565120726472,"user_tz":-540,"elapsed":3900,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["dataset_sizes = {x:len(stem_datasets[x]) for x in ['train', 'test']}\n","dataset_sizes"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'test': 120, 'train': 480}"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"lHln9RRZ2p_t","colab_type":"text"},"source":["## DataLoader 로 넘겨주기"]},{"cell_type":"code","metadata":{"id":"2kBx1MOAESqo","colab_type":"code","outputId":"85bb644f-8828-4944-a7e3-3b0bf3338ca5","executionInfo":{"status":"ok","timestamp":1565120726472,"user_tz":-540,"elapsed":3243,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# dataset을 딥러닝 학습할 데이터 셋으로 만들어 준다.... \n","# batch_size를 지정할 수 있다.\n","# shuffle을 True로 하는게 좋다. batch돌면서 랜덤하게 데이터를 준다.\n","# num_workers가 0이면 모든 코어를 다 쓴다?는거 같음.\n","\n","# DataLoader한 후에 train, test를 나눌 수 없다. train,test를 split한 다음에 DataLoader로 넘겨줘야한다.\n","\n","dataloaders = {x: torch.utils.data.DataLoader(stem_datasets[x], batch_size=4, shuffle=True, num_workers=4) \\\n","              for x in ['train', 'test']}\n","dataloaders"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'test': <torch.utils.data.dataloader.DataLoader at 0x7f741368b390>,\n"," 'train': <torch.utils.data.dataloader.DataLoader at 0x7f741368b0f0>}"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"WBgirOZTmCqo","colab_type":"code","colab":{}},"source":["# 이건 연습용 코드\n","# 위와 같은 내용임.\n","\n","# train = torch.utils.data.DataLoader(stem_images_train, batch_size=4, shuffle=True, num_workers=0)\n","# test = torch.utils.data.DataLoader(stem_images_test, batch_size=4, shuffle=False, num_workers=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFggAQA71qcp","colab_type":"code","outputId":"4f803336-0906-4a06-f125-4324de178048","executionInfo":{"status":"ok","timestamp":1565120726474,"user_tz":-540,"elapsed":2701,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# DataLoader 하기 전과 type이 바뀜.\n","\n","type(dataloaders['train'])"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.utils.data.dataloader.DataLoader"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"vHK0qMgQ4Ue-","colab_type":"text"},"source":["# vgg pre-trained 가져오기"]},{"cell_type":"code","metadata":{"id":"LjJDhT8r4FAs","colab_type":"code","colab":{}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7o5KOStk3yjs","colab_type":"code","colab":{}},"source":["vgg_model = torchvision.models.vgg16_bn(pretrained=True).to(device) # 기존에 만들어진 vgg network 을 이미지넷 데이터에 트레이닝해둔 파라미터를 그대로 받아옵니다."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgX5jliC4zef","colab_type":"code","outputId":"e72b0520-d83d-4bc3-fb84-aabcd8c7142a","executionInfo":{"status":"ok","timestamp":1565120733593,"user_tz":-540,"elapsed":8262,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["vgg_model"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace)\n","    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace)\n","    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): ReLU(inplace)\n","    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (16): ReLU(inplace)\n","    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (19): ReLU(inplace)\n","    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (22): ReLU(inplace)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (26): ReLU(inplace)\n","    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (29): ReLU(inplace)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (32): ReLU(inplace)\n","    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (36): ReLU(inplace)\n","    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (39): ReLU(inplace)\n","    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (42): ReLU(inplace)\n","    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace)\n","    (2): Dropout(p=0.5)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace)\n","    (5): Dropout(p=0.5)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"COQJfh8w5ds_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":826},"outputId":"5eec391c-f5bc-4efb-c973-bd4eaf7fc454","executionInfo":{"status":"ok","timestamp":1565120733595,"user_tz":-540,"elapsed":7870,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["# vgg_model.features 는 vgg모델에서 feature extraction을 담당하는 부분만 sequential된 부분이다.\n","# children()하면 generator로 바꿔준다.\n","\n","[x for x in vgg_model.features.children()]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n"," Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n"," Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n"," Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n"," Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n"," BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace),\n"," MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"kpATHvOX4_f0","colab_type":"text"},"source":["# 학습모델"]},{"cell_type":"code","metadata":{"id":"vJ9lTPT9n1dM","colab_type":"code","colab":{}},"source":["def train_network(net,optimizer,trainloader, epochs=5):\n","  for epoch in range(epochs):  # loop over the dataset multiple times\n","                               # epochs = 전체 데이터가 한바퀴 돈 것\n","      running_loss = 0.0 # running loss를 저장하기 위한 변수입니다. \n","      for i, data in enumerate(trainloader): # 한 Epoch 만큼 돕니다. 매 iteration 마다 정해진 Batch size 만큼 데이터를 뱉습니다. \n","          # get the inputs\n","          inputs, labels = data # DataLoader iterator의 반환 값은 input_data 와 labels의 튜플 형식입니다. \n","          inputs = inputs.to(device) # Pytorch에서 nn.Module 에 넣어 Backprop을 계산 하기 위해서는 gpu 연동을 이와 같이 해줘야 합니다.\n","          labels = labels.to(device)\n","          # zero the parameter gradients\n","          optimizer.zero_grad()    #  현재 기존의 backprop을 계산하기 위해서 저장했던 activation buffer 를 비웁니다.\n","          # forward + backward + optimize\n","          outputs = net(inputs) # input 을 넣은 위 network 로 부터 output 을 얻어냅니다. \n","          loss = criterion(outputs, labels) # loss fucntion에 주어진 target과 output 의 score를 계산하여 반환합니다. \n","          loss.backward() # * Scalar Loss value를 Backward() 해주게 되면 주어진 loss값을 바탕으로 backpropagation이 진행됩니다. \n","          optimizer.step() # 계산된 Backprop 을 바탕으로 optimizer가 gradient descenting 을 수행합니다. \n","\n","          # print statistics\n","          running_loss += loss.item()\n","          if (i+1) % 10 == 0:    # print every 500 mini-batches\n","              print('[%d, %5d] loss: %.3f' %\n","                    (epoch + 1, i + 1, running_loss / 100))\n","              running_loss = 0.0\n","\n","  print('Finished Training')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3F3ZmR6-oIH3","colab_type":"code","colab":{}},"source":["def test(model,test_loader):\n","  model.eval() # Eval Mode 왜 해야 할까요?  --> nn.Dropout BatchNorm 등의 Regularization 들이 test 모드로 들어가게 되기 때문입니다. \n","  test_loss = 0\n","  correct = 0\n","  for data, target in test_loader:\n","    data = data.to(device)\n","    target = target.to(device)  # 기존의 train function의 data 처리부분과 같습니다. \n","    output = model(data) \n","    pred = output.max(1, keepdim=True)[1] # get the index of the max \n","    correct += pred.eq(target.view_as(pred)).sum().item() # 정답 데이터의 갯수를 반환합니다. \n","\n","  test_loss /= len(test_loader.dataset)\n","  print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","      correct, len(test_loader.dataset),\n","      100. * correct / len(test_loader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eiRvdEd4u9D","colab_type":"code","colab":{}},"source":["class DiyCNN(nn.Module):\n","    def __init__(self, vgg_model):\n","        super(DiyCNN, self).__init__()\n","        self.pre_trained = nn.Sequential(   \n","            *list(vgg_model.features.children()) # vgg_model의 features에 있는 모든 레이어 (children)들을 차례로 가져와서 붙여줍니다.\n","        ) # 14x14x512\n","        \n","        # fc는 내가 필요한대로 새로 만들어야 한다.\n","        self.mlp = nn.Sequential(  # 기존에는 이미지넷에 학습되어있기 때문에, 이를 내 데이터셋용으로 바꿔줄 필요가 있습니다. \n","            nn.Linear(25088, 4096),   # 따라서 1000이 아닌 10가지의 클래스만을 대상으로 하는 linear 레이어를 새로 쌓고 학습시켜주는부분입니다.\n","            nn.ReLU(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Linear(4096, 3),\n","        )\n","    def forward(self, x):\n","        out = self.pre_trained(x) # 512,7,7\n","        out = out.view(out.size(0), -1)\n","        out = self.mlp(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jK2VVR9_tFe","colab_type":"code","colab":{}},"source":["def count_parameters(model): # 모델 파라미터 개수를 리턴하는 함수를 하나 만들어둡니다\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oA1AZt8o_GMU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"9af99346-b824-42d3-8060-75ff2707fcc4","executionInfo":{"status":"ok","timestamp":1565123448697,"user_tz":-540,"elapsed":2045,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["lr = 0.001\n","stem_net = DiyCNN(vgg_model).to(device)\n","criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n","optimizer = optim.Adam(stem_net.parameters(), lr=lr) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. \n","\n","print('[info] number of model parameter - %d'%(count_parameters(stem_net))) # 방금 구성한 모형의 파라미터 개수를 프린트 해봅니다."],"execution_count":66,"outputs":[{"output_type":"stream","text":["[info] number of model parameter - 134281283\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9WLER5NaAKZI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":256},"outputId":"e480380e-0c29-478c-ff2b-547a830ff503","executionInfo":{"status":"ok","timestamp":1565121985479,"user_tz":-540,"elapsed":117210,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["# 1 epochs\n","\n","train_network(stem_net,optimizer,dataloaders[\"train\"], epochs=1)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["[1,    10] loss: 0.236\n","[1,    20] loss: 0.085\n","[1,    30] loss: 0.090\n","[1,    40] loss: 0.091\n","[1,    50] loss: 0.068\n","[1,    60] loss: 0.054\n","[1,    70] loss: 0.066\n","[1,    80] loss: 0.050\n","[1,    90] loss: 0.024\n","[1,   100] loss: 0.089\n","[1,   110] loss: 0.078\n","[1,   120] loss: 0.066\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9c9Y_G8_AZ2K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"6d37b61e-5b7e-4f66-cd5d-69db43211403","executionInfo":{"status":"ok","timestamp":1565122013949,"user_tz":-540,"elapsed":134234,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["# 성능 출중하네......\n","\n","test(stem_net,dataloaders[\"test\"])"],"execution_count":42,"outputs":[{"output_type":"stream","text":["\n","Test set:  Accuracy: 114/120 (95%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KbpjClIhhnrw","colab_type":"code","colab":{}},"source":["torch.save(stem_net.state_dict(), \"/drive/My Drive/Colab Notebooks/pre_vgf16.pth\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yq7iEAG6hno_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"590cfda3-7f84-4cc5-cca2-9834f9499834","executionInfo":{"status":"ok","timestamp":1565121985479,"user_tz":-540,"elapsed":117210,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"id":"JWx-tk3qntNu","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 200 epochs 자기전에 돌려보자..\n","\n","train_network(stem_net,optimizer,dataloaders[\"train\"], epochs=200)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,    10] loss: 0.434\n","[1,    20] loss: 0.138\n","[1,    30] loss: 0.124\n","[1,    40] loss: 0.076\n","[1,    50] loss: 0.094\n","[1,    60] loss: 0.100\n","[1,    70] loss: 0.079\n","[1,    80] loss: 0.055\n","[1,    90] loss: 0.072\n","[1,   100] loss: 0.079\n","[1,   110] loss: 0.054\n","[1,   120] loss: 0.043\n","[2,    10] loss: 0.130\n","[2,    20] loss: 0.087\n","[2,    30] loss: 0.084\n","[2,    40] loss: 0.083\n","[2,    50] loss: 0.075\n","[2,    60] loss: 0.061\n","[2,    70] loss: 0.021\n","[2,    80] loss: 0.041\n","[2,    90] loss: 0.094\n","[2,   100] loss: 0.058\n","[2,   110] loss: 0.145\n","[2,   120] loss: 0.153\n","[3,    10] loss: 0.097\n","[3,    20] loss: 0.071\n","[3,    30] loss: 0.093\n","[3,    40] loss: 0.073\n","[3,    50] loss: 0.118\n","[3,    60] loss: 0.092\n","[3,    70] loss: 0.061\n","[3,    80] loss: 0.081\n","[3,    90] loss: 0.142\n","[3,   100] loss: 0.088\n","[3,   110] loss: 0.165\n","[3,   120] loss: 0.099\n","[4,    10] loss: 0.120\n","[4,    20] loss: 0.147\n","[4,    30] loss: 0.084\n","[4,    40] loss: 0.131\n","[4,    50] loss: 0.107\n","[4,    60] loss: 0.104\n","[4,    70] loss: 0.099\n","[4,    80] loss: 0.103\n","[4,    90] loss: 0.100\n","[4,   100] loss: 0.094\n","[4,   110] loss: 0.084\n","[4,   120] loss: 0.091\n","[5,    10] loss: 0.088\n","[5,    20] loss: 0.094\n","[5,    30] loss: 0.081\n","[5,    40] loss: 0.091\n","[5,    50] loss: 0.137\n","[5,    60] loss: 0.087\n","[5,    70] loss: 0.082\n","[5,    80] loss: 0.068\n","[5,    90] loss: 0.085\n","[5,   100] loss: 0.067\n","[5,   110] loss: 0.078\n","[5,   120] loss: 0.063\n","[6,    10] loss: 0.057\n","[6,    20] loss: 0.059\n","[6,    30] loss: 0.080\n","[6,    40] loss: 0.116\n","[6,    50] loss: 0.068\n","[6,    60] loss: 0.054\n","[6,    70] loss: 0.082\n","[6,    80] loss: 0.047\n","[6,    90] loss: 0.072\n","[6,   100] loss: 0.074\n","[6,   110] loss: 0.027\n","[6,   120] loss: 0.098\n","[7,    10] loss: 0.074\n","[7,    20] loss: 0.166\n","[7,    30] loss: 0.071\n","[7,    40] loss: 0.046\n","[7,    50] loss: 0.034\n","[7,    60] loss: 0.074\n","[7,    70] loss: 0.068\n","[7,    80] loss: 0.053\n","[7,    90] loss: 0.038\n","[7,   100] loss: 0.124\n","[7,   110] loss: 0.076\n","[7,   120] loss: 0.055\n","[8,    10] loss: 0.042\n","[8,    20] loss: 0.063\n","[8,    30] loss: 0.028\n","[8,    40] loss: 0.124\n","[8,    50] loss: 0.025\n","[8,    60] loss: 0.055\n","[8,    70] loss: 0.193\n","[8,    80] loss: 0.033\n","[8,    90] loss: 0.054\n","[8,   100] loss: 0.050\n","[8,   110] loss: 0.044\n","[8,   120] loss: 0.048\n","[9,    10] loss: 0.065\n","[9,    20] loss: 0.079\n","[9,    30] loss: 0.042\n","[9,    40] loss: 0.112\n","[9,    50] loss: 0.051\n","[9,    60] loss: 0.064\n","[9,    70] loss: 0.036\n","[9,    80] loss: 0.062\n","[9,    90] loss: 0.294\n","[9,   100] loss: 0.113\n","[9,   110] loss: 0.075\n","[9,   120] loss: 0.063\n","[10,    10] loss: 0.059\n","[10,    20] loss: 0.039\n","[10,    30] loss: 0.036\n","[10,    40] loss: 0.177\n","[10,    50] loss: 0.064\n","[10,    60] loss: 0.074\n","[10,    70] loss: 0.082\n","[10,    80] loss: 0.045\n","[10,    90] loss: 0.051\n","[10,   100] loss: 0.057\n","[10,   110] loss: 0.083\n","[10,   120] loss: 0.047\n","[11,    10] loss: 0.056\n","[11,    20] loss: 0.039\n","[11,    30] loss: 0.055\n","[11,    40] loss: 0.042\n","[11,    50] loss: 0.053\n","[11,    60] loss: 0.045\n","[11,    70] loss: 0.119\n","[11,    80] loss: 0.064\n","[11,    90] loss: 0.043\n","[11,   100] loss: 0.058\n","[11,   110] loss: 0.054\n","[11,   120] loss: 0.038\n","[12,    10] loss: 0.116\n","[12,    20] loss: 0.031\n","[12,    30] loss: 0.047\n","[12,    40] loss: 0.075\n","[12,    50] loss: 0.050\n","[12,    60] loss: 0.062\n","[12,    70] loss: 0.032\n","[12,    80] loss: 0.038\n","[12,    90] loss: 0.033\n","[12,   100] loss: 0.063\n","[12,   110] loss: 0.029\n","[12,   120] loss: 0.018\n","[13,    10] loss: 0.049\n","[13,    20] loss: 0.015\n","[13,    30] loss: 0.012\n","[13,    40] loss: 0.011\n","[13,    50] loss: 0.027\n","[13,    60] loss: 0.029\n","[13,    70] loss: 0.033\n","[13,    80] loss: 0.044\n","[13,    90] loss: 0.132\n","[13,   100] loss: 0.046\n","[13,   110] loss: 0.033\n","[13,   120] loss: 0.042\n","[14,    10] loss: 0.067\n","[14,    20] loss: 0.038\n","[14,    30] loss: 0.048\n","[14,    40] loss: 0.022\n","[14,    50] loss: 0.029\n","[14,    60] loss: 0.025\n","[14,    70] loss: 0.025\n","[14,    80] loss: 0.085\n","[14,    90] loss: 0.032\n","[14,   100] loss: 0.037\n","[14,   110] loss: 0.059\n","[14,   120] loss: 0.035\n","[15,    10] loss: 0.041\n","[15,    20] loss: 0.022\n","[15,    30] loss: 0.021\n","[15,    40] loss: 0.014\n","[15,    50] loss: 0.043\n","[15,    60] loss: 0.023\n","[15,    70] loss: 0.036\n","[15,    80] loss: 0.019\n","[15,    90] loss: 0.022\n","[15,   100] loss: 0.020\n","[15,   110] loss: 0.026\n","[15,   120] loss: 0.032\n","[16,    10] loss: 0.171\n","[16,    20] loss: 0.112\n","[16,    30] loss: 0.045\n","[16,    40] loss: 0.014\n","[16,    50] loss: 0.069\n","[16,    60] loss: 0.025\n","[16,    70] loss: 0.028\n","[16,    80] loss: 0.019\n","[16,    90] loss: 0.022\n","[16,   100] loss: 0.026\n","[16,   110] loss: 0.020\n","[16,   120] loss: 0.020\n","[17,    10] loss: 0.016\n","[17,    20] loss: 0.037\n","[17,    30] loss: 0.051\n","[17,    40] loss: 0.042\n","[17,    50] loss: 0.041\n","[17,    60] loss: 0.032\n","[17,    70] loss: 0.025\n","[17,    80] loss: 0.036\n","[17,    90] loss: 0.019\n","[17,   100] loss: 0.017\n","[17,   110] loss: 0.016\n","[17,   120] loss: 0.030\n","[18,    10] loss: 0.021\n","[18,    20] loss: 0.023\n","[18,    30] loss: 0.039\n","[18,    40] loss: 0.029\n","[18,    50] loss: 0.010\n","[18,    60] loss: 0.011\n","[18,    70] loss: 0.010\n","[18,    80] loss: 0.025\n","[18,    90] loss: 0.017\n","[18,   100] loss: 0.058\n","[18,   110] loss: 0.038\n","[18,   120] loss: 0.026\n","[19,    10] loss: 0.014\n","[19,    20] loss: 0.014\n","[19,    30] loss: 0.033\n","[19,    40] loss: 0.042\n","[19,    50] loss: 0.009\n","[19,    60] loss: 0.017\n","[19,    70] loss: 0.009\n","[19,    80] loss: 0.007\n","[19,    90] loss: 0.084\n","[19,   100] loss: 0.023\n","[19,   110] loss: 0.021\n","[19,   120] loss: 0.018\n","[20,    10] loss: 0.016\n","[20,    20] loss: 0.017\n","[20,    30] loss: 0.013\n","[20,    40] loss: 0.014\n","[20,    50] loss: 0.028\n","[20,    60] loss: 0.033\n","[20,    70] loss: 0.044\n","[20,    80] loss: 0.021\n","[20,    90] loss: 0.026\n","[20,   100] loss: 0.027\n","[20,   110] loss: 0.050\n","[20,   120] loss: 0.032\n","[21,    10] loss: 0.015\n","[21,    20] loss: 0.008\n","[21,    30] loss: 0.054\n","[21,    40] loss: 0.027\n","[21,    50] loss: 0.027\n","[21,    60] loss: 0.021\n","[21,    70] loss: 0.009\n","[21,    80] loss: 0.029\n","[21,    90] loss: 0.014\n","[21,   100] loss: 0.018\n","[21,   110] loss: 0.017\n","[21,   120] loss: 0.013\n","[22,    10] loss: 0.005\n","[22,    20] loss: 0.030\n","[22,    30] loss: 0.012\n","[22,    40] loss: 0.021\n","[22,    50] loss: 0.044\n","[22,    60] loss: 0.022\n","[22,    70] loss: 0.007\n","[22,    80] loss: 0.017\n","[22,    90] loss: 0.002\n","[22,   100] loss: 0.020\n","[22,   110] loss: 0.018\n","[22,   120] loss: 0.021\n","[23,    10] loss: 0.010\n","[23,    20] loss: 0.012\n","[23,    30] loss: 0.005\n","[23,    40] loss: 0.120\n","[23,    50] loss: 0.063\n","[23,    60] loss: 0.023\n","[23,    70] loss: 0.032\n","[23,    80] loss: 0.018\n","[23,    90] loss: 0.025\n","[23,   100] loss: 0.024\n","[23,   110] loss: 0.016\n","[23,   120] loss: 0.013\n","[24,    10] loss: 0.022\n","[24,    20] loss: 0.033\n","[24,    30] loss: 0.018\n","[24,    40] loss: 0.014\n","[24,    50] loss: 0.038\n","[24,    60] loss: 0.013\n","[24,    70] loss: 0.009\n","[24,    80] loss: 0.022\n","[24,    90] loss: 0.019\n","[24,   100] loss: 0.007\n","[24,   110] loss: 0.012\n","[24,   120] loss: 0.035\n","[25,    10] loss: 0.004\n","[25,    20] loss: 0.005\n","[25,    30] loss: 0.011\n","[25,    40] loss: 0.009\n","[25,    50] loss: 0.003\n","[25,    60] loss: 0.030\n","[25,    70] loss: 0.007\n","[25,    80] loss: 0.007\n","[25,    90] loss: 0.029\n","[25,   100] loss: 0.010\n","[25,   110] loss: 0.006\n","[25,   120] loss: 0.024\n","[26,    10] loss: 0.013\n","[26,    20] loss: 0.006\n","[26,    30] loss: 0.004\n","[26,    40] loss: 0.020\n","[26,    50] loss: 0.003\n","[26,    60] loss: 0.022\n","[26,    70] loss: 0.008\n","[26,    80] loss: 0.027\n","[26,    90] loss: 0.012\n","[26,   100] loss: 0.014\n","[26,   110] loss: 0.019\n","[26,   120] loss: 0.017\n","[27,    10] loss: 0.008\n","[27,    20] loss: 0.023\n","[27,    30] loss: 0.030\n","[27,    40] loss: 0.006\n","[27,    50] loss: 0.015\n","[27,    60] loss: 0.004\n","[27,    70] loss: 0.015\n","[27,    80] loss: 0.004\n","[27,    90] loss: 0.003\n","[27,   100] loss: 0.011\n","[27,   110] loss: 0.012\n","[27,   120] loss: 0.014\n","[28,    10] loss: 0.005\n","[28,    20] loss: 0.028\n","[28,    30] loss: 0.001\n","[28,    40] loss: 0.010\n","[28,    50] loss: 0.012\n","[28,    60] loss: 0.004\n","[28,    70] loss: 0.006\n","[28,    80] loss: 0.003\n","[28,    90] loss: 0.068\n","[28,   100] loss: 0.013\n","[28,   110] loss: 0.023\n","[28,   120] loss: 0.016\n","[29,    10] loss: 0.020\n","[29,    20] loss: 0.086\n","[29,    30] loss: 0.037\n","[29,    40] loss: 0.021\n","[29,    50] loss: 0.044\n","[29,    60] loss: 0.031\n","[29,    70] loss: 0.038\n","[29,    80] loss: 0.026\n","[29,    90] loss: 0.028\n","[29,   100] loss: 0.024\n","[29,   110] loss: 0.030\n","[29,   120] loss: 0.010\n","[30,    10] loss: 0.008\n","[30,    20] loss: 0.014\n","[30,    30] loss: 0.025\n","[30,    40] loss: 0.035\n","[30,    50] loss: 0.015\n","[30,    60] loss: 0.011\n","[30,    70] loss: 0.018\n","[30,    80] loss: 0.010\n","[30,    90] loss: 0.017\n","[30,   100] loss: 0.004\n","[30,   110] loss: 0.010\n","[30,   120] loss: 0.004\n","[31,    10] loss: 0.010\n","[31,    20] loss: 0.002\n","[31,    30] loss: 0.014\n","[31,    40] loss: 0.053\n","[31,    50] loss: 0.013\n","[31,    60] loss: 0.023\n","[31,    70] loss: 0.185\n","[31,    80] loss: 0.021\n","[31,    90] loss: 0.012\n","[31,   100] loss: 0.038\n","[31,   110] loss: 0.021\n","[31,   120] loss: 0.019\n","[32,    10] loss: 0.008\n","[32,    20] loss: 0.017\n","[32,    30] loss: 0.007\n","[32,    40] loss: 0.026\n","[32,    50] loss: 0.065\n","[32,    60] loss: 0.041\n","[32,    70] loss: 0.027\n","[32,    80] loss: 0.046\n","[32,    90] loss: 0.122\n","[32,   100] loss: 0.023\n","[32,   110] loss: 0.019\n","[32,   120] loss: 0.129\n","[33,    10] loss: 0.030\n","[33,    20] loss: 0.055\n","[33,    30] loss: 0.016\n","[33,    40] loss: 0.059\n","[33,    50] loss: 0.024\n","[33,    60] loss: 0.012\n","[33,    70] loss: 0.026\n","[33,    80] loss: 0.014\n","[33,    90] loss: 0.010\n","[33,   100] loss: 0.049\n","[33,   110] loss: 0.049\n","[33,   120] loss: 0.046\n","[34,    10] loss: 0.026\n","[34,    20] loss: 0.013\n","[34,    30] loss: 0.049\n","[34,    40] loss: 0.012\n","[34,    50] loss: 0.003\n","[34,    60] loss: 0.015\n","[34,    70] loss: 0.038\n","[34,    80] loss: 0.017\n","[34,    90] loss: 0.006\n","[34,   100] loss: 0.004\n","[34,   110] loss: 0.012\n","[34,   120] loss: 0.033\n","[35,    10] loss: 0.082\n","[35,    20] loss: 0.023\n","[35,    30] loss: 0.009\n","[35,    40] loss: 0.012\n","[35,    50] loss: 0.018\n","[35,    60] loss: 0.005\n","[35,    70] loss: 0.007\n","[35,    80] loss: 0.003\n","[35,    90] loss: 0.008\n","[35,   100] loss: 0.060\n","[35,   110] loss: 0.017\n","[35,   120] loss: 0.019\n","[36,    10] loss: 0.011\n","[36,    20] loss: 0.007\n","[36,    30] loss: 0.030\n","[36,    40] loss: 0.041\n","[36,    50] loss: 0.025\n","[36,    60] loss: 0.023\n","[36,    70] loss: 0.013\n","[36,    80] loss: 0.017\n","[36,    90] loss: 0.025\n","[36,   100] loss: 0.012\n","[36,   110] loss: 0.010\n","[36,   120] loss: 0.031\n","[37,    10] loss: 0.017\n","[37,    20] loss: 0.030\n","[37,    30] loss: 0.148\n","[37,    40] loss: 0.016\n","[37,    50] loss: 0.024\n","[37,    60] loss: 0.015\n","[37,    70] loss: 0.022\n","[37,    80] loss: 0.020\n","[37,    90] loss: 0.021\n","[37,   100] loss: 0.011\n","[37,   110] loss: 0.006\n","[37,   120] loss: 0.021\n","[38,    10] loss: 0.036\n","[38,    20] loss: 0.012\n","[38,    30] loss: 0.019\n","[38,    40] loss: 0.019\n","[38,    50] loss: 0.007\n","[38,    60] loss: 0.014\n","[38,    70] loss: 0.056\n","[38,    80] loss: 0.001\n","[38,    90] loss: 0.033\n","[38,   100] loss: 0.021\n","[38,   110] loss: 0.013\n","[38,   120] loss: 0.012\n","[39,    10] loss: 0.032\n","[39,    20] loss: 0.004\n","[39,    30] loss: 0.008\n","[39,    40] loss: 0.001\n","[39,    50] loss: 0.005\n","[39,    60] loss: 0.009\n","[39,    70] loss: 0.004\n","[39,    80] loss: 0.002\n","[39,    90] loss: 0.036\n","[39,   100] loss: 0.017\n","[39,   110] loss: 0.024\n","[39,   120] loss: 0.004\n","[40,    10] loss: 0.007\n","[40,    20] loss: 0.001\n","[40,    30] loss: 0.001\n","[40,    40] loss: 0.008\n","[40,    50] loss: 0.001\n","[40,    60] loss: 0.029\n","[40,    70] loss: 0.003\n","[40,    80] loss: 0.050\n","[40,    90] loss: 0.066\n","[40,   100] loss: 0.012\n","[40,   110] loss: 0.024\n","[40,   120] loss: 0.013\n","[41,    10] loss: 0.008\n","[41,    20] loss: 0.016\n","[41,    30] loss: 0.001\n","[41,    40] loss: 0.014\n","[41,    50] loss: 0.011\n","[41,    60] loss: 0.001\n","[41,    70] loss: 0.001\n","[41,    80] loss: 0.002\n","[41,    90] loss: 0.000\n","[41,   100] loss: 0.014\n","[41,   110] loss: 0.021\n","[41,   120] loss: 0.021\n","[42,    10] loss: 0.031\n","[42,    20] loss: 0.044\n","[42,    30] loss: 0.030\n","[42,    40] loss: 0.028\n","[42,    50] loss: 0.018\n","[42,    60] loss: 0.010\n","[42,    70] loss: 0.016\n","[42,    80] loss: 0.036\n","[42,    90] loss: 0.010\n","[42,   100] loss: 0.018\n","[42,   110] loss: 0.018\n","[42,   120] loss: 0.010\n","[43,    10] loss: 0.021\n","[43,    20] loss: 0.006\n","[43,    30] loss: 0.007\n","[43,    40] loss: 0.020\n","[43,    50] loss: 0.033\n","[43,    60] loss: 0.018\n","[43,    70] loss: 0.023\n","[43,    80] loss: 0.010\n","[43,    90] loss: 0.029\n","[43,   100] loss: 0.033\n","[43,   110] loss: 0.009\n","[43,   120] loss: 0.067\n","[44,    10] loss: 0.004\n","[44,    20] loss: 0.013\n","[44,    30] loss: 0.012\n","[44,    40] loss: 0.010\n","[44,    50] loss: 0.005\n","[44,    60] loss: 0.033\n","[44,    70] loss: 0.004\n","[44,    80] loss: 0.021\n","[44,    90] loss: 0.006\n","[44,   100] loss: 0.008\n","[44,   110] loss: 0.015\n","[44,   120] loss: 0.026\n","[45,    10] loss: 0.005\n","[45,    20] loss: 0.014\n","[45,    30] loss: 0.013\n","[45,    40] loss: 0.005\n","[45,    50] loss: 0.005\n","[45,    60] loss: 0.008\n","[45,    70] loss: 0.003\n","[45,    80] loss: 0.005\n","[45,    90] loss: 0.006\n","[45,   100] loss: 0.005\n","[45,   110] loss: 0.005\n","[45,   120] loss: 0.007\n","[46,    10] loss: 0.016\n","[46,    20] loss: 0.018\n","[46,    30] loss: 0.008\n","[46,    40] loss: 0.007\n","[46,    50] loss: 0.005\n","[46,    60] loss: 0.006\n","[46,    70] loss: 0.018\n","[46,    80] loss: 0.042\n","[46,    90] loss: 0.032\n","[46,   100] loss: 0.019\n","[46,   110] loss: 0.027\n","[46,   120] loss: 0.005\n","[47,    10] loss: 0.004\n","[47,    20] loss: 0.021\n","[47,    30] loss: 0.006\n","[47,    40] loss: 0.003\n","[47,    50] loss: 0.019\n","[47,    60] loss: 0.002\n","[47,    70] loss: 0.008\n","[47,    80] loss: 0.008\n","[47,    90] loss: 0.056\n","[47,   100] loss: 0.039\n","[47,   110] loss: 0.007\n","[47,   120] loss: 0.014\n","[48,    10] loss: 0.009\n","[48,    20] loss: 0.003\n","[48,    30] loss: 0.004\n","[48,    40] loss: 0.011\n","[48,    50] loss: 0.008\n","[48,    60] loss: 0.009\n","[48,    70] loss: 0.011\n","[48,    80] loss: 0.002\n","[48,    90] loss: 0.005\n","[48,   100] loss: 0.006\n","[48,   110] loss: 0.006\n","[48,   120] loss: 0.002\n","[49,    10] loss: 0.000\n","[49,    20] loss: 0.000\n","[49,    30] loss: 0.028\n","[49,    40] loss: 0.007\n","[49,    50] loss: 0.014\n","[49,    60] loss: 0.007\n","[49,    70] loss: 0.015\n","[49,    80] loss: 0.003\n","[49,    90] loss: 0.002\n","[49,   100] loss: 0.027\n","[49,   110] loss: 0.016\n","[49,   120] loss: 0.039\n","[50,    10] loss: 0.008\n","[50,    20] loss: 0.004\n","[50,    30] loss: 0.015\n","[50,    40] loss: 0.009\n","[50,    50] loss: 0.002\n","[50,    60] loss: 0.315\n","[50,    70] loss: 0.050\n","[50,    80] loss: 0.038\n","[50,    90] loss: 0.043\n","[50,   100] loss: 0.020\n","[50,   110] loss: 0.029\n","[50,   120] loss: 0.085\n","[51,    10] loss: 0.040\n","[51,    20] loss: 0.029\n","[51,    30] loss: 0.019\n","[51,    40] loss: 0.015\n","[51,    50] loss: 0.013\n","[51,    60] loss: 0.005\n","[51,    70] loss: 0.039\n","[51,    80] loss: 0.660\n","[51,    90] loss: 0.023\n","[51,   100] loss: 0.111\n","[51,   110] loss: 0.043\n","[51,   120] loss: 0.047\n","[52,    10] loss: 0.030\n","[52,    20] loss: 0.058\n","[52,    30] loss: 0.033\n","[52,    40] loss: 0.122\n","[52,    50] loss: 0.102\n","[52,    60] loss: 0.055\n","[52,    70] loss: 0.046\n","[52,    80] loss: 0.041\n","[52,    90] loss: 0.023\n","[52,   100] loss: 0.034\n","[52,   110] loss: 0.037\n","[52,   120] loss: 0.022\n","[53,    10] loss: 0.017\n","[53,    20] loss: 0.029\n","[53,    30] loss: 0.033\n","[53,    40] loss: 0.063\n","[53,    50] loss: 0.046\n","[53,    60] loss: 0.034\n","[53,    70] loss: 0.030\n","[53,    80] loss: 0.023\n","[53,    90] loss: 0.013\n","[53,   100] loss: 0.012\n","[53,   110] loss: 0.038\n","[53,   120] loss: 0.021\n","[54,    10] loss: 0.021\n","[54,    20] loss: 0.008\n","[54,    30] loss: 0.007\n","[54,    40] loss: 0.027\n","[54,    50] loss: 0.021\n","[54,    60] loss: 0.006\n","[54,    70] loss: 0.006\n","[54,    80] loss: 0.013\n","[54,    90] loss: 0.002\n","[54,   100] loss: 0.033\n","[54,   110] loss: 0.007\n","[54,   120] loss: 0.026\n","[55,    10] loss: 0.035\n","[55,    20] loss: 0.007\n","[55,    30] loss: 0.084\n","[55,    40] loss: 0.004\n","[55,    50] loss: 0.011\n","[55,    60] loss: 0.026\n","[55,    70] loss: 0.018\n","[55,    80] loss: 0.045\n","[55,    90] loss: 0.124\n","[55,   100] loss: 0.010\n","[55,   110] loss: 0.018\n","[55,   120] loss: 0.077\n","[56,    10] loss: 0.009\n","[56,    20] loss: 0.023\n","[56,    30] loss: 0.068\n","[56,    40] loss: 0.022\n","[56,    50] loss: 0.007\n","[56,    60] loss: 0.013\n","[56,    70] loss: 0.003\n","[56,    80] loss: 0.010\n","[56,    90] loss: 0.005\n","[56,   100] loss: 0.017\n","[56,   110] loss: 0.025\n","[56,   120] loss: 0.005\n","[57,    10] loss: 0.023\n","[57,    20] loss: 0.008\n","[57,    30] loss: 0.016\n","[57,    40] loss: 0.029\n","[57,    50] loss: 0.020\n","[57,    60] loss: 0.005\n","[57,    70] loss: 0.003\n","[57,    80] loss: 0.007\n","[57,    90] loss: 0.001\n","[57,   100] loss: 0.017\n","[57,   110] loss: 0.019\n","[57,   120] loss: 0.002\n","[58,    10] loss: 0.003\n","[58,    20] loss: 0.008\n","[58,    30] loss: 0.001\n","[58,    40] loss: 0.005\n","[58,    50] loss: 0.005\n","[58,    60] loss: 0.003\n","[58,    70] loss: 0.003\n","[58,    80] loss: 0.002\n","[58,    90] loss: 0.002\n","[58,   100] loss: 0.008\n","[58,   110] loss: 0.003\n","[58,   120] loss: 0.002\n","[59,    10] loss: 0.000\n","[59,    20] loss: 0.000\n","[59,    30] loss: 0.001\n","[59,    40] loss: 0.000\n","[59,    50] loss: 0.024\n","[59,    60] loss: 0.001\n","[59,    70] loss: 0.005\n","[59,    80] loss: 0.010\n","[59,    90] loss: 0.002\n","[59,   100] loss: 0.001\n","[59,   110] loss: 0.002\n","[59,   120] loss: 0.000\n","[60,    10] loss: 0.007\n","[60,    20] loss: 0.002\n","[60,    30] loss: 0.002\n","[60,    40] loss: 0.004\n","[60,    50] loss: 0.007\n","[60,    60] loss: 0.026\n","[60,    70] loss: 0.004\n","[60,    80] loss: 0.003\n","[60,    90] loss: 0.001\n","[60,   100] loss: 0.001\n","[60,   110] loss: 0.006\n","[60,   120] loss: 0.005\n","[61,    10] loss: 0.000\n","[61,    20] loss: 0.002\n","[61,    30] loss: 0.001\n","[61,    40] loss: 0.000\n","[61,    50] loss: 0.000\n","[61,    60] loss: 0.000\n","[61,    70] loss: 0.000\n","[61,    80] loss: 0.001\n","[61,    90] loss: 0.000\n","[61,   100] loss: 0.006\n","[61,   110] loss: 0.020\n","[61,   120] loss: 0.011\n","[62,    10] loss: 0.005\n","[62,    20] loss: 0.001\n","[62,    30] loss: 0.001\n","[62,    40] loss: 0.001\n","[62,    50] loss: 0.004\n","[62,    60] loss: 0.001\n","[62,    70] loss: 0.001\n","[62,    80] loss: 0.001\n","[62,    90] loss: 0.002\n","[62,   100] loss: 0.000\n","[62,   110] loss: 0.001\n","[62,   120] loss: 0.000\n","[63,    10] loss: 0.011\n","[63,    20] loss: 0.003\n","[63,    30] loss: 0.001\n","[63,    40] loss: 0.003\n","[63,    50] loss: 0.001\n","[63,    60] loss: 0.001\n","[63,    70] loss: 0.000\n","[63,    80] loss: 0.001\n","[63,    90] loss: 0.000\n","[63,   100] loss: 0.000\n","[63,   110] loss: 0.002\n","[63,   120] loss: 0.003\n","[64,    10] loss: 0.000\n","[64,    20] loss: 0.000\n","[64,    30] loss: 0.009\n","[64,    40] loss: 0.004\n","[64,    50] loss: 0.000\n","[64,    60] loss: 0.001\n","[64,    70] loss: 0.000\n","[64,    80] loss: 0.000\n","[64,    90] loss: 0.000\n","[64,   100] loss: 0.000\n","[64,   110] loss: 0.000\n","[64,   120] loss: 0.000\n","[65,    10] loss: 0.000\n","[65,    20] loss: 0.000\n","[65,    30] loss: 0.009\n","[65,    40] loss: 0.009\n","[65,    50] loss: 0.006\n","[65,    60] loss: 0.011\n","[65,    70] loss: 0.037\n","[65,    80] loss: 0.003\n","[65,    90] loss: 0.021\n","[65,   100] loss: 0.018\n","[65,   110] loss: 0.001\n","[65,   120] loss: 0.007\n","[66,    10] loss: 0.008\n","[66,    20] loss: 0.000\n","[66,    30] loss: 0.002\n","[66,    40] loss: 0.002\n","[66,    50] loss: 0.004\n","[66,    60] loss: 0.000\n","[66,    70] loss: 0.001\n","[66,    80] loss: 0.001\n","[66,    90] loss: 0.001\n","[66,   100] loss: 0.001\n","[66,   110] loss: 0.000\n","[66,   120] loss: 0.040\n","[67,    10] loss: 0.003\n","[67,    20] loss: 0.008\n","[67,    30] loss: 0.003\n","[67,    40] loss: 0.000\n","[67,    50] loss: 0.003\n","[67,    60] loss: 0.009\n","[67,    70] loss: 0.029\n","[67,    80] loss: 0.001\n","[67,    90] loss: 0.001\n","[67,   100] loss: 0.040\n","[67,   110] loss: 0.000\n","[67,   120] loss: 0.005\n","[68,    10] loss: 0.003\n","[68,    20] loss: 0.005\n","[68,    30] loss: 0.001\n","[68,    40] loss: 0.002\n","[68,    50] loss: 0.000\n","[68,    60] loss: 0.002\n","[68,    70] loss: 0.001\n","[68,    80] loss: 0.001\n","[68,    90] loss: 0.000\n","[68,   100] loss: 0.000\n","[68,   110] loss: 0.000\n","[68,   120] loss: 0.000\n","[69,    10] loss: 0.000\n","[69,    20] loss: 0.018\n","[69,    30] loss: 0.001\n","[69,    40] loss: 0.118\n","[69,    50] loss: 0.047\n","[69,    60] loss: 0.009\n","[69,    70] loss: 0.067\n","[69,    80] loss: 0.010\n","[69,    90] loss: 0.026\n","[69,   100] loss: 0.008\n","[69,   110] loss: 0.023\n","[69,   120] loss: 0.020\n","[70,    10] loss: 0.006\n","[70,    20] loss: 0.002\n","[70,    30] loss: 0.002\n","[70,    40] loss: 0.002\n","[70,    50] loss: 0.002\n","[70,    60] loss: 0.000\n","[70,    70] loss: 0.000\n","[70,    80] loss: 0.006\n","[70,    90] loss: 0.003\n","[70,   100] loss: 0.001\n","[70,   110] loss: 0.000\n","[70,   120] loss: 0.006\n","[71,    10] loss: 0.007\n","[71,    20] loss: 0.013\n","[71,    30] loss: 0.003\n","[71,    40] loss: 0.004\n","[71,    50] loss: 0.004\n","[71,    60] loss: 0.003\n","[71,    70] loss: 0.026\n","[71,    80] loss: 0.004\n","[71,    90] loss: 0.007\n","[71,   100] loss: 0.002\n","[71,   110] loss: 0.001\n","[71,   120] loss: 0.001\n","[72,    10] loss: 0.000\n","[72,    20] loss: 0.004\n","[72,    30] loss: 0.003\n","[72,    40] loss: 0.000\n","[72,    50] loss: 0.000\n","[72,    60] loss: 0.000\n","[72,    70] loss: 0.000\n","[72,    80] loss: 0.000\n","[72,    90] loss: 0.001\n","[72,   100] loss: 0.000\n","[72,   110] loss: 0.000\n","[72,   120] loss: 0.000\n","[73,    10] loss: 0.000\n","[73,    20] loss: 0.000\n","[73,    30] loss: 0.000\n","[73,    40] loss: 0.000\n","[73,    50] loss: 0.000\n","[73,    60] loss: 0.000\n","[73,    70] loss: 0.000\n","[73,    80] loss: 0.002\n","[73,    90] loss: 0.001\n","[73,   100] loss: 0.000\n","[73,   110] loss: 0.000\n","[73,   120] loss: 0.000\n","[74,    10] loss: 0.000\n","[74,    20] loss: 0.009\n","[74,    30] loss: 0.097\n","[74,    40] loss: 0.008\n","[74,    50] loss: 0.012\n","[74,    60] loss: 0.006\n","[74,    70] loss: 0.003\n","[74,    80] loss: 0.001\n","[74,    90] loss: 0.007\n","[74,   100] loss: 0.053\n","[74,   110] loss: 0.022\n","[74,   120] loss: 0.006\n","[75,    10] loss: 0.002\n","[75,    20] loss: 0.004\n","[75,    30] loss: 0.001\n","[75,    40] loss: 0.003\n","[75,    50] loss: 0.017\n","[75,    60] loss: 0.015\n","[75,    70] loss: 0.018\n","[75,    80] loss: 0.009\n","[75,    90] loss: 0.007\n","[75,   100] loss: 0.004\n","[75,   110] loss: 0.003\n","[75,   120] loss: 0.002\n","[76,    10] loss: 0.001\n","[76,    20] loss: 0.003\n","[76,    30] loss: 0.003\n","[76,    40] loss: 0.000\n","[76,    50] loss: 0.002\n","[76,    60] loss: 0.000\n","[76,    70] loss: 0.000\n","[76,    80] loss: 0.000\n","[76,    90] loss: 0.000\n","[76,   100] loss: 0.000\n","[76,   110] loss: 0.000\n","[76,   120] loss: 0.002\n","[77,    10] loss: 0.000\n","[77,    20] loss: 0.000\n","[77,    30] loss: 0.001\n","[77,    40] loss: 0.001\n","[77,    50] loss: 0.000\n","[77,    60] loss: 0.001\n","[77,    70] loss: 0.000\n","[77,    80] loss: 0.000\n","[77,    90] loss: 0.000\n","[77,   100] loss: 0.000\n","[77,   110] loss: 0.000\n","[77,   120] loss: 0.000\n","[78,    10] loss: 0.000\n","[78,    20] loss: 0.000\n","[78,    30] loss: 0.000\n","[78,    40] loss: 0.000\n","[78,    50] loss: 0.000\n","[78,    60] loss: 0.000\n","[78,    70] loss: 0.000\n","[78,    80] loss: 0.001\n","[78,    90] loss: 0.000\n","[78,   100] loss: 0.000\n","[78,   110] loss: 0.000\n","[78,   120] loss: 0.000\n","[79,    10] loss: 0.000\n","[79,    20] loss: 0.000\n","[79,    30] loss: 0.003\n","[79,    40] loss: 0.000\n","[79,    50] loss: 0.025\n","[79,    60] loss: 0.003\n","[79,    70] loss: 0.025\n","[79,    80] loss: 0.011\n","[79,    90] loss: 0.017\n","[79,   100] loss: 0.037\n","[79,   110] loss: 0.023\n","[79,   120] loss: 0.006\n","[80,    10] loss: 0.006\n","[80,    20] loss: 0.002\n","[80,    30] loss: 0.002\n","[80,    40] loss: 0.001\n","[80,    50] loss: 0.001\n","[80,    60] loss: 0.001\n","[80,    70] loss: 0.003\n","[80,    80] loss: 0.010\n","[80,    90] loss: 0.016\n","[80,   100] loss: 0.002\n","[80,   110] loss: 0.002\n","[80,   120] loss: 0.003\n","[81,    10] loss: 0.001\n","[81,    20] loss: 0.000\n","[81,    30] loss: 0.001\n","[81,    40] loss: 0.007\n","[81,    50] loss: 0.004\n","[81,    60] loss: 0.008\n","[81,    70] loss: 0.003\n","[81,    80] loss: 0.012\n","[81,    90] loss: 0.002\n","[81,   100] loss: 0.006\n","[81,   110] loss: 0.000\n","[81,   120] loss: 0.010\n","[82,    10] loss: 0.001\n","[82,    20] loss: 0.002\n","[82,    30] loss: 0.002\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"fc8d1afd-be52-4c5e-d6a6-607aa3efc834","executionInfo":{"status":"error","timestamp":1565134654136,"user_tz":-540,"elapsed":2156,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"id":"q5TnSqk4ntNx","colab":{"base_uri":"https://localhost:8080/","height":174}},"source":["test(stem_net,dataloaders[\"test\"])"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1586dac33837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5TgomEl8ntN0","colab":{"base_uri":"https://localhost:8080/","height":174},"outputId":"8130a22c-7d92-4319-eff8-37d78a05396b","executionInfo":{"status":"error","timestamp":1565134654138,"user_tz":-540,"elapsed":2148,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["torch.save(stem_net.state_dict(), \"/drive/My Drive/Colab Notebooks/pre_vgf16_with_200e.pth\")"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fc027584260b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/drive/My Drive/Colab Notebooks/pre_vgf16_with_200e.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ZtSuVRbDhp0m","colab_type":"text"},"source":["# 시행착오"]},{"cell_type":"code","metadata":{"id":"uPLb1EvvE5EM","colab_type":"code","colab":{}},"source":["# pre_trained = nn.Sequential(*list(vgg_model.features.children()) )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OwLweJmhFc86","colab_type":"code","colab":{}},"source":["# datas, classes = next(iter(dataloaders['train']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SW8bqmd-FIA3","colab_type":"code","colab":{}},"source":["# p = pre_trained(datas)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7JFmZnPLBjI","colab_type":"code","outputId":"cc0bcf2b-bead-4560-cb4f-1a6665905fc7","executionInfo":{"status":"ok","timestamp":1565115831992,"user_tz":-540,"elapsed":9224,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# p[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([512, 7, 7])"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"-6h8BS9VLUNy","colab_type":"code","outputId":"4247bc54-005c-46f6-d74a-0f28a75c9299","executionInfo":{"status":"ok","timestamp":1565116380964,"user_tz":-540,"elapsed":596,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# datas[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 224, 224])"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"XIe85r7NLUEk","colab_type":"code","colab":{}},"source":["# mlp = nn.Sequential(  # 기존에는 이미지넷에 학습되어있기 때문에, 이를 내 데이터셋용으로 바꿔줄 필요가 있습니다. \n","#             nn.Linear(7, 14336),   # 따라서 1000이 아닌 10가지의 클래스만을 대상으로 하는 linear 레이어를 새로 쌓고 학습시켜주는부분입니다.\n","#             nn.ReLU(),\n","#             nn.Linear(4096, 4096),\n","#             nn.ReLU(),\n","#             nn.Linear(4096, 3),)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqcAGs6lMRLN","colab_type":"code","outputId":"8cd09381-2033-41c7-cff3-1c024436ef9a","executionInfo":{"status":"error","timestamp":1565116477843,"user_tz":-540,"elapsed":3059,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}},"colab":{"base_uri":"https://localhost:8080/","height":346}},"source":["# m = mlp(p)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-76-32c256831473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [14336 x 14336], m2: [4096 x 4096] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961"]}]},{"cell_type":"markdown","metadata":{"id":"KfywpMDThunS","colab_type":"text"},"source":["# Fixed"]},{"cell_type":"code","metadata":{"id":"d5AgfkKzkOaX","colab_type":"code","colab":{}},"source":["fixed_vgg_model = torchvision.models.vgg16_bn(pretrained=True).to(device) # 기존에 만들어진 vgg network 을 이미지넷 데이터에 트레이닝해둔 파라미터를 그대로 받아옵니다."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSpMw3v8hnl-","colab_type":"code","colab":{}},"source":["for parameter in fixed_vgg_model.parameters():\n","    parameter.requires_grad = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KX1BIxmzlKfI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"outputId":"9e7bbbf2-be3c-4bd4-8ee0-24b426be953a","executionInfo":{"status":"ok","timestamp":1565122848847,"user_tz":-540,"elapsed":868,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["fixed_vgg_model.classifier"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=25088, out_features=4096, bias=True)\n","  (1): ReLU(inplace)\n","  (2): Dropout(p=0.5)\n","  (3): Linear(in_features=4096, out_features=4096, bias=True)\n","  (4): ReLU(inplace)\n","  (5): Dropout(p=0.5)\n","  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"b1nNjuuEk_ej","colab_type":"code","colab":{}},"source":["fixed_vgg_model.classifier = nn.Sequential(\n","    nn.Linear(25088, 4096),\n","    nn.ReLU(),\n","    nn.Linear(4096, 4096),\n","    nn.ReLU(),\n","    nn.Linear(4096,3)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ve_TOABtmSrX","colab_type":"code","colab":{}},"source":["fixed_vgg_model = fixed_vgg_model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIg4f4L-muXL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":256},"outputId":"f4d1ec7a-47d8-429b-8ee0-0525b539bdc4","executionInfo":{"status":"ok","timestamp":1565123207784,"user_tz":-540,"elapsed":114203,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["train_network(stem_net,optimizer,dataloaders[\"train\"], epochs=1)"],"execution_count":60,"outputs":[{"output_type":"stream","text":["[1,    10] loss: 0.140\n","[1,    20] loss: 0.112\n","[1,    30] loss: 0.109\n","[1,    40] loss: 0.110\n","[1,    50] loss: 0.111\n","[1,    60] loss: 0.110\n","[1,    70] loss: 0.111\n","[1,    80] loss: 0.110\n","[1,    90] loss: 0.110\n","[1,   100] loss: 0.110\n","[1,   110] loss: 0.110\n","[1,   120] loss: 0.110\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pWLS6BvCmyP1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"e2ce3fc3-225d-475d-a2e1-b1f17b1fd0cd","executionInfo":{"status":"ok","timestamp":1565123296181,"user_tz":-540,"elapsed":27695,"user":{"displayName":"신진효","photoUrl":"https://lh5.googleusercontent.com/-boMtsUluo44/AAAAAAAAAAI/AAAAAAAAKQY/ROXwV35uGuk/s64/photo.jpg","userId":"08312626099912519803"}}},"source":["test(fixed_vgg_model,dataloaders[\"test\"])"],"execution_count":63,"outputs":[{"output_type":"stream","text":["\n","Test set:  Accuracy: 41/120 (34%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZZdBInaGm2OM","colab_type":"code","colab":{}},"source":["torch.save(fixed_vgg_model.state_dict(), \"/drive/My Drive/Colab Notebooks/fixed_vgf16.pth\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKDNCTH2m57L","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}